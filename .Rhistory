data = subset(data, data$Quantity == 1)
data = subset(data, ! data$AdoptionSpeed == 4)
# Only pulling the columns we want to look at
dat = data[c('Type', 'Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt','VideoAmt', 'AdoptionSpeed')]
summary(dat)
# Adding random values
#dat$ASnum = NaN
#dat$ASnum[dat$AdoptionSpeed == 0] = 0
#dat$ASnum[dat$AdoptionSpeed == 1] = floor(runif(sum(dat$AdoptionSpeed == 1), 1, 8))
#dat$ASnum[dat$AdoptionSpeed == 2] = floor(runif(sum(dat$AdoptionSpeed == 2), 8, 31))
#dat$ASnum[dat$AdoptionSpeed == 3] = floor(runif(sum(dat$AdoptionSpeed == 3), 31, 91))
#dat$ASnum[dat$AdoptionSpeed == 4] = 100 -- this line isn't necessary because we exclude animals that have this adoption speed
summary(dat)
datclean = dat[, c('Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed')]
datclean$y = dat[, c('Type')]
loadPkg("bestglm")
res.bestglm <- bestglm(Xy = datclean, family = binomial,
IC = "AIC",                 # Information criteria for
method = "exhaustive")
summary(res.bestglm)
res.bestglm$BestModels
summary(res.bestglm$BestModels)
unloadPkg("bestglm")
unloadPkg("leaps") # leaps is required by bestglm, thus cannot be unloaded before bestglm
typeLogit <- glm(Type ~ Age + Gender + MaturitySize + FurLength + Vaccinated + AdoptionSpeed + PhotoAmt, data = dat, family = "binomial")
xkabledply(typeLogit, title = paste("Logistic Regression :", format(formula(typeLogit)) ))
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
typeHoslem = hoslem.test(dat$Type, fitted(typeLogit)) # Hosmer and Lemeshow test, a chi-squared test
typeHoslem
unloadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(typeLogit, type = "response" )
dat$prob=prob
h <- roc(Type~prob, data=dat)
plot(h)
auc(h)
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
typePR = pR2(typeLogit)
typePR
unloadPkg("pscl")
datage <- dat[ ,c('Type','Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed') ]
datage$puppy = NaN
datage$puppy[dat$Age <= 3] = 1
datage$puppy[dat$Age > 3] = 0
datage$puppy = as.factor(datage$puppy)
puppies = subset(datage, datage$puppy == 1)
older = subset(datage, datage$puppy ==0)
puppy_proportion = nrow(puppies) * 100/nrow(dat)
loadPkg("bestglm")
datage$y = datage$puppy
datage <- subset(datage, -c('puppy'))
loadPkg("bestglm")
datage$y = datage$puppy
datage <- subset(datage, ! datage$puppy)
logit_pups <- bestglm(Xy =datage, family = binomial, IC = "AIC", method = "exhaustive")
loadPkg("bestglm")
datage$y = datage$puppy
datage <- subset(datage, ! datage$puppy)
datage <- na.omit(datage)
logit_pups <- bestglm(Xy =datage, family = binomial, IC = "AIC", method = "exhaustive")
loadPkg("bestglm")
loadPkg("leaps")
datage$y = datage$puppy
datage <- subset(datage, ! datage$puppy)
datage <- na.omit(datage)
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- subset(datage, ! datage$puppy)
datage <- na.omit(datage)
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- na.omit(datage)
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F, echo = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
library("corrplot")
library("ezids")
library("gtsummary")
library('ggridges')
library('viridis')
library('wesanderson')
library('gridExtra')
library('leaps')
library('vtable')
## Loading data
data = read.csv('datafile.csv')
initialrows = nrow(data)
data$AdoptionSpeed <- factor(data$AdoptionSpeed, order=T, levels = c(0,1,2,3,4))
# converting the data types we want - categorical
data$Type = as.factor(data$Type)
data$MaturitySize = as.factor(data$MaturitySize)
data$FurLength = as.factor(data$FurLength)
data$Vaccinated = as.factor(data$Vaccinated)
data$Gender = as.factor(data$Gender)
# Only looking at profiles with 1 animal
data = subset(data, data$Quantity == 1)
data = subset(data, ! data$AdoptionSpeed == 4)
# Only pulling the columns we want to look at
dat = data[c('Type', 'Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt','VideoAmt', 'AdoptionSpeed')]
summary(dat)
# Adding random values
#dat$ASnum = NaN
#dat$ASnum[dat$AdoptionSpeed == 0] = 0
#dat$ASnum[dat$AdoptionSpeed == 1] = floor(runif(sum(dat$AdoptionSpeed == 1), 1, 8))
#dat$ASnum[dat$AdoptionSpeed == 2] = floor(runif(sum(dat$AdoptionSpeed == 2), 8, 31))
#dat$ASnum[dat$AdoptionSpeed == 3] = floor(runif(sum(dat$AdoptionSpeed == 3), 31, 91))
#dat$ASnum[dat$AdoptionSpeed == 4] = 100 -- this line isn't necessary because we exclude animals that have this adoption speed
summary(dat)
datclean = dat[, c('Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed')]
datclean$y = dat[, c('Type')]
loadPkg("bestglm")
res.bestglm <- bestglm(Xy = datclean, family = binomial,
IC = "AIC",                 # Information criteria for
method = "exhaustive")
summary(res.bestglm)
res.bestglm$BestModels
summary(res.bestglm$BestModels)
unloadPkg("bestglm")
unloadPkg("leaps") # leaps is required by bestglm, thus cannot be unloaded before bestglm
typeLogit <- glm(Type ~ Age + Gender + MaturitySize + FurLength + Vaccinated + AdoptionSpeed + PhotoAmt, data = dat, family = "binomial")
xkabledply(typeLogit, title = paste("Logistic Regression :", format(formula(typeLogit)) ))
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
typeHoslem = hoslem.test(dat$Type, fitted(typeLogit)) # Hosmer and Lemeshow test, a chi-squared test
typeHoslem
unloadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(typeLogit, type = "response" )
dat$prob=prob
h <- roc(Type~prob, data=dat)
plot(h)
auc(h)
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
typePR = pR2(typeLogit)
typePR
unloadPkg("pscl")
datage <- dat[ ,c('Type','Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed') ]
datage$puppy = NaN
datage$puppy[dat$Age <= 3] = 1
datage$puppy[dat$Age > 3] = 0
datage$puppy = as.factor(datage$puppy)
puppies = subset(datage, datage$puppy == 1)
older = subset(datage, datage$puppy ==0)
puppy_proportion = nrow(puppies) * 100/nrow(dat)
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- na.omit(datage)
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- na.omit(datage)
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
summary(logit_pups)
logit_pups$BestModels
summary(logit_pups$BestModels)
loadPkg("rpart") # Classification trees, rpart(formula, data=, method=,control=)
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
# The package "ezids" (EZ Intro to Data Science) includes a lot of the helper functions we developed for the course.
# Some of the frequently used functions are loadPkg(), xkabledply(), xkablesummary(), uzscale(), etc.
library(ezids)
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
loadPkg("rpart") # Classification trees, rpart(formula, data=, method=,control=)
loadPkg("MASS")
pima <- rbind.data.frame(Pima.te,Pima.tr)
str(pima)
xkablesummary(pima, title = "Pima dataset five-number summary")
loadPkg("psych")
pairs(pima)
pairs.panels(pima,
method = "pearson", # correlation method
hist.col = "#00AFBB", # set histogram color, can use "#22AFBB", "red",
density = TRUE,  # show density plots
ellipses = TRUE # show correlation ellipses
)
unloadPkg(psych)
scaledpima <- as.data.frame(scale(pima[1:7], center = TRUE, scale = TRUE))
set.seed(321)
pima_sample <- sample(2, nrow(scaledpima), replace=TRUE, prob=c(0.75, 0.25))
train_X <- scaledpima[pima_sample==1, 1:7]
test_X <- scaledpima[pima_sample==2, 1:7]
train_y <- pima[pima_sample==1, 8]
test_y <- pima[pima_sample==2, 8]
loadPkg("FNN")
loadPkg("gmodels") # CrossTable
loadPkg("caret") # confusionMatrix
ResultDf = NULL
for (kval in 3:11) {
pima_pred <- knn(train = train_X, test = test_X, cl=train_y, k=kval)
PimaCross <- CrossTable(test_y, pima_pred, prop.chisq = FALSE)
print( paste("k = ", kval) )
PimaCross
#
cm = confusionMatrix(pima_pred, reference = test_y ) # from caret library
# print.confusionMatrix(cm)
#
cmaccu = cm$overall['Accuracy']
print( paste("Total Accuracy = ", cmaccu ) )
# print("Other metrics : ")
# print(cm$byClass)
#
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
# cmt = cbind( cmt, data.frame( t(cm$byClass) ) ) # the dataframe of the transpose, with k valued added in front
ResultDf = rbind(ResultDf, cmt)
print( xkabledply(   as.matrix(cm), title = paste("ConfusionMatrix for k = ",kval ) ) )
print( xkabledply(data.frame(cm$byClass), title=paste("k = ",kval)) )
}
xkabledply(ResultDf, title = "Total Accuracy Summary")
xkabledply(prop.table(table(test_y)), wide = TRUE, title = "y-target prop")
idx_max_acc = which.max(ResultDf$Total.Accuracy)
# re-do that one with max accuracy
pima_pred_m <- knn(train = train_X, test = test_X, cl=train_y, k=ResultDf[idx_max_acc,1])
PimaCross_m <- CrossTable(test_y, pima_pred_m, prop.chisq = FALSE)
unloadPkg("FNN")
unloadPkg("gmodels")
loadPkg("caret")
cmknn = confusionMatrix(pima_pred_m, reference = test_y )
print('Overall: ')
cmknn$overall
print('Class: ')
cmknn$byClass
unloadPkg("caret")
pimaLogit <- glm(train_y ~ ., data = train_X, family = "binomial")
summary( pimaLogit )
xkabledply( pimaLogit )
pima_logitpredict.5 <- predict(pimaLogit, newdata = test_X, type = "response") > 0.5
pima_logitpredict.5 <- ifelse(pima_logitpredict.5,"Yes","No")  # replace TRUE with "Yes", and FALSE to "No"
crossTable.5 = table(pima_logitpredict.5,test_y)
xkabledply(crossTable.5, title = "Cross table for Logit Regression at 0.5 cutoff")
# crossTable
loadPkg("caret")
cmLogit5 = confusionMatrix( factor(pima_logitpredict.5), reference = test_y )
print('Overall: ')
cmLogit5$overall
print('Class: ')
cmLogit5$byClass
unloadPkg("caret")
pima_logitpredict.35 <- predict(pimaLogit, newdata = test_X, type = "response") > 0.35
pima_logitpredict.35 <- ifelse(pima_logitpredict.35,"Yes","No")  # replace TRUE with "Yes", and FALSE to "No"
#pima_predict
crossTable.35 = table(pima_logitpredict.35,test_y)
xkabledply(crossTable.35, title = "Cross table for Logit Regression at 0.5 cutoff")
# crossTable
loadPkg("caret")
cmLogit = confusionMatrix( factor(pima_logitpredict.35), reference = test_y )
print('Overall: ')
cmLogit$overall
print('Class: ')
cmLogit$byClass
unloadPkg("caret")
loadPkg("regclass")
confusion_matrix(pimaLogit)
unloadPkg("regclass")
loadPkg("pROC")
prob=predict(pimaLogit, type = "response" )
h <- roc(train_y~prob)
auc(h)
plot(h)
unloadPkg("pROC")
loadPkg("pROC")
h <- roc(test_y , as.integer(pima_pred_m))
auc(h)
plot(h)
unloadPkg("pROC")
loadPkg("rpart") # Classification trees, rpart(formula, data=, method=,control=)
set.seed(1)
pimatree <- rpart(train_y ~ . , data=train_X , method="class", control = list(maxdepth = 6 ) )
# rpart.control(maxdepth = 30, minsplit = 20, minbucket = round(minsplit/3), cp = 0.01, maxcompete = 4, maxsurrogate = 5, usesurrogate = 2, xval = 10, surrogatestyle = 0, ...)
printcp(pimatree) # display the results
plotcp(pimatree) # visualize cross-validation results
summary(pimatree) # detailed summary of splits
# plot tree
plot(pimatree, uniform=TRUE, main="Classification Tree for Pima")
text(pimatree, use.n=TRUE, all=TRUE, cex=.8)
pima_treepredict <- predict(pimatree, newdata = test_X, type = "class")
crossTable.tree = table(pima_treepredict,test_y)
xkabledply(crossTable.tree, title = "Cross table for Tree model")
# crossTable
pima_treepredict <- predict(pimatree, newdata = test_X, type = "class")
crossTable.tree = table(pima_treepredict,test_y)
xkabledply(crossTable.tree, title = "Cross table for Tree model")
# crossTable
loadPkg("caret")
cmTree = confusionMatrix( predict(pimatree, newdata = test_X, type = "class"), reference = test_y )
print('Overall: ')
cmTree$overall
print('Class: ')
cmTree$byClass
unloadPkg("caret")
loadPkg("pROC")
pima_treepredictprob <- predict(pimatree, newdata = test_X, type = "prob")
h <- roc (test_y, pima_treepredictprob[,2])
auc(h)
plot(h)
unloadPkg("pROC")
# some of common options (and the defaults) are:
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right',
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F, echo = F)
options(scientific=T, digits = 3)
# options(scipen=9, digits = 3)
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
library("corrplot")
library("ezids")
library("gtsummary")
library('ggridges')
library('viridis')
library('wesanderson')
library('gridExtra')
library('leaps')
library('vtable')
library('pscl')
## Loading data
data = read.csv('datafile.csv')
initialrows = nrow(data)
data$AdoptionSpeed <- factor(data$AdoptionSpeed, order=T, levels = c(0,1,2,3,4))
# converting the data types we want - categorical
data$Type = as.factor(data$Type)
data$MaturitySize = as.factor(data$MaturitySize)
data$FurLength = as.factor(data$FurLength)
data$Vaccinated = as.factor(data$Vaccinated)
data$Gender = as.factor(data$Gender)
# Only looking at profiles with 1 animal
data = subset(data, data$Quantity == 1)
data = subset(data, ! data$AdoptionSpeed == 4)
# Only pulling the columns we want to look at
dat = data[c('Type', 'Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt','VideoAmt', 'AdoptionSpeed')]
summary(dat)
# Adding random values
#dat$ASnum = NaN
#dat$ASnum[dat$AdoptionSpeed == 0] = 0
#dat$ASnum[dat$AdoptionSpeed == 1] = floor(runif(sum(dat$AdoptionSpeed == 1), 1, 8))
#dat$ASnum[dat$AdoptionSpeed == 2] = floor(runif(sum(dat$AdoptionSpeed == 2), 8, 31))
#dat$ASnum[dat$AdoptionSpeed == 3] = floor(runif(sum(dat$AdoptionSpeed == 3), 31, 91))
#dat$ASnum[dat$AdoptionSpeed == 4] = 100 -- this line isn't necessary because we exclude animals that have this adoption speed
summary(dat)
# creating a cleaned dataset without the animal type
datclean = dat[, c('Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed')]
# labeling the target variable appropriately
datclean$y = dat[, c('Type')]
str(datclean)
loadPkg("bestglm")
# feature selection
res.bestglm <- bestglm(Xy = datclean, family = binomial,
IC = "AIC",
method = "exhaustive")
#summary(res.bestglm) # printing the summary
res.bestglm$BestModels
#summary(res.bestglm$BestModels)
unloadPkg("bestglm")
typeLogit <- glm(y ~ Age + Gender + MaturitySize + FurLength + Vaccinated + AdoptionSpeed + PhotoAmt, data = datclean, family = "binomial")
xkabledply(typeLogit, title = paste("Logistic Regression : Type ~ age+gender+size+fur+vaccinated+AdoptSpd+photos"))
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
typeHoslem = hoslem.test(datclean$y, fitted(typeLogit)) # Hosmer and Lemeshow test, a chi-squared test
typeHoslem
unloadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(typeLogit, type = "response" )
datclean$prob=prob
h <- roc(y~prob, data=datclean)
plot(h)
auc(h)
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
typePR = pR2(typeLogit)
typePR
unloadPkg("pscl")
loadPkg("gmodels")
# reloading the original dataframe to cancel all the factor variables
knndat = read.csv('datafile.csv')
knndat = subset(knndat, knndat$Quantity == 1)
knndat = subset(knndat, ! knndat$AdoptionSpeed == 4)
knndat = knndat[c('Type', 'Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt','VideoAmt', 'AdoptionSpeed')]
scaledknn = as.data.frame(scale(knndat[c(2, 7, 8)], center = TRUE, scale = TRUE)) # only scaling the numerical values
scaledknn[c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'AdoptionSpeed')] = knndat[c(1, 3, 4, 5, 6, 9)]
#sampling for train and test
set.seed(1000)
knn_sample <- sample(2, nrow(scaledknn), replace=TRUE, prob=c(0.7, 0.3))
train <- scaledknn[knn_sample==1, 1:3, 5:9]
test <- scaledknn[knn_sample==2, 1:3, 5:9]
typeknn.trainLabel = scaledknn[knn_sample==1, 4]
typeknn.testLabel = scaledknn[knn_sample==2, 4]
ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )
for (kval in 3:20) {
tpred <- knn(train = train, test = test, cl=typeknn.trainLabel, k=kval)
cross <- CrossTable(typeknn.testLabel, tpred, prop.chisq = FALSE)
cm = confusionMatrix(tpred, reference = as.factor(typeknn.testLabel)) # from caret library
cmaccu = cm$overall['Accuracy']
cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics
ResultDf = rbind(ResultDf, cmt)
}
datage <- dat[ ,c('Type','Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed') ]
datage$puppy = NaN
datage$puppy[dat$Age <= 3] = 1
datage$puppy[dat$Age > 3] = 0
datage$puppy = as.factor(datage$puppy)
puppies = subset(datage, datage$puppy == 1)
older = subset(datage, datage$puppy ==0)
puppy_proportion = nrow(puppies) * 100/nrow(dat)
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- na.omit(datage)
# using bestglm as it has feature selection built in
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
summary(logit_pups)
unloadPkg("bestglm")
logit_pups$BestModels
summary(logit_pups$BestModels)
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- datage[, c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed')]
datage <- na.omit(datage)
# using bestglm as it has feature selection built in
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
View(datage)
View(datage)
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- datage[, c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed', 'y')]
datage <- dat[ ,c('Type','Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed') ]
datage$puppy = NaN
datage$puppy[dat$Age <= 3] = 1
datage$puppy[dat$Age > 3] = 0
datage$puppy = as.factor(datage$puppy)
puppies = subset(datage, datage$puppy == 1)
older = subset(datage, datage$puppy ==0)
puppy_proportion = nrow(puppies) * 100/nrow(dat)
View(datage)
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- datage[, c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed', 'y')]
datage <- na.omit(datage)
# using bestglm as it has feature selection built in
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
summary(logit_pups)
unloadPkg("bestglm")
logit_pups$BestModels
summary(logit_pups$BestModels)
pup_logit <- glm(y ~ Type + Gender + MaturitySize + FurLength + Vaccinated + VideoAmt + AdoptionSpeed, data = datage, family = "binomial")
pup_logit <- glm(y ~ Type + Gender + MaturitySize + FurLength + Vaccinated + VideoAmt + AdoptionSpeed, data = datage, family = "binomial")
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
ageHoslem = hoslem.test(datage$y, fitted(pup_logit)) # Hosmer and Lemeshow test, a chi-squared test
ageHoslem
unloadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
ageHoslem = hoslem.test(datage$y, fitted(pup_logit)) # Hosmer and Lemeshow test, a chi-squared test
ageHoslem
unloadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
loadPkg("pROC")
prob=predict(pup_logit, type = "response" )
datage$prob=prob
h <- roc(y~prob, data=datage)
plot(h)
auc(h)
loadPkg("rpart")
puppy_tree <- rpart(y ~ .,data = datage, method="class", control = list(maxdepth = 4))
printcp(kyphosisfit) # display the results
printcp(puppy_tree) # display the results
plotcp(puppy_tree) # visualize cross-validation results
summary(puppy_tree) # detailed summary of splits
View(datage)
loadPkg("rpart")
datage <- datage[, c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed', 'y')]
puppy_tree <- rpart(y ~ .,data = datage, method="class", control = list(maxdepth = 4))
printcp(puppy_tree) # display the results
plotcp(puppy_tree) # visualize cross-validation results
summary(puppy_tree) # detailed summary of splits
View(puppies)
loadPkg("rpart")
datage <- datage[, c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed', 'y')]
puppy_tree <- rpart(y ~ .,data = datage, method="class", control = list(maxdepth = 4))
View(datage)
printcp(puppy_tree) # display the results
plotcp(puppy_tree) # visualize cross-validation results
summary(puppy_tree) # detailed summary of splits
plot(puppy_tree, uniform=TRUE, main="Classification Tree for Puppy")
text(puppy_tree, use.n=TRUE, all=TRUE, cex=.8)
plot(puppy_tree, uniform=TRUE, main="Classification Tree for Puppy")
text(puppy_tree, use.n=TRUE, all=TRUE, cex=.8)
