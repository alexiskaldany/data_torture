---
title: "PetFinder Adoption Final Paper"
author: "Group 3: Data Torturers (Alexis Kaldany, Sahara Ensley, Yixi Liang, Kaiyuan Liang)"
date: "`r Sys.Date()`"
output: 
  rmdformats::robobook:
  html_document:
    code_folding: hide
    number_sections: false
    toc: yes
    toc_depth: 3
    toc_float: yes
  pdf_document:
    toc: yes
    toc_depth: '3'
---

```{r setup, include=FALSE}
# some of common options (and the defaults) are: 
# include=T, eval=T, echo=T, results='hide'/'asis'/'markup',..., collapse=F, warning=T, message=T, error=T, cache=T, fig.width=6, fig.height=4, fig.dim=c(6,4) #inches, fig.align='left'/'center','right', 
library(ezids)
# knitr::opts_chunk$set(warning = F, results = "markup", message = F)
knitr::opts_chunk$set(warning = F, results = "hide", message = F, echo = F)
options(scientific=T, digits = 3) 
# options(scipen=9, digits = 3) 
# ‘scipen’: integer. A penalty to be applied when deciding to print numeric values in fixed or exponential notation.  Positive values bias towards fixed and negative towards scientific notation: fixed notation will be preferred unless it is more than ‘scipen’ digits wider.
# use scipen=999 to prevent scientific notation at all times
```

- Loading data and libraries
```{r}
library("corrplot")
library("ezids")
library("gtsummary")
library('ggridges')
library('viridis')
library('wesanderson')
library('gridExtra')
library('leaps')
library('vtable')

## Loading data
data = read.csv('datafile.csv')

initialrows = nrow(data)

data$AdoptionSpeed <- factor(data$AdoptionSpeed, order=T, levels = c(0,1,2,3,4))

# converting the data types we want - categorical
data$Type = as.factor(data$Type)
data$MaturitySize = as.factor(data$MaturitySize)
data$FurLength = as.factor(data$FurLength)
data$Vaccinated = as.factor(data$Vaccinated)
data$Gender = as.factor(data$Gender)

# Only looking at profiles with 1 animal
data = subset(data, data$Quantity == 1)
data = subset(data, ! data$AdoptionSpeed == 4)

# Only pulling the columns we want to look at
dat = data[c('Type', 'Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt','VideoAmt', 'AdoptionSpeed')]
summary(dat)

# Adding random values
#dat$ASnum = NaN
#dat$ASnum[dat$AdoptionSpeed == 0] = 0
#dat$ASnum[dat$AdoptionSpeed == 1] = floor(runif(sum(dat$AdoptionSpeed == 1), 1, 8))
#dat$ASnum[dat$AdoptionSpeed == 2] = floor(runif(sum(dat$AdoptionSpeed == 2), 8, 31))
#dat$ASnum[dat$AdoptionSpeed == 3] = floor(runif(sum(dat$AdoptionSpeed == 3), 31, 91))
#dat$ASnum[dat$AdoptionSpeed == 4] = 100 -- this line isn't necessary because we exclude animals that have this adoption speed

summary(dat)
```

#Kaiyuan's section




# Yixi's section




# Sahara's section
# SMART QUESTION: Can the type of animal be classified from the adoption profile?

Given that the type of animal is a categorical variable this is a classification problem. The first model we can try is a Logistic Regression model. First we need a new clean dataset with the features we want and a renamed target variable for the feature selection.

```{r, results = 'markup'}
# creating a cleaned dataset without the animal type
datclean = dat[, c('Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed')]

# labeling the target variable appropriately
datclean$y = dat[, c('Type')]
str(datclean)
```

Now we can run the feature selection.
```{r, results='markup'}
loadPkg("bestglm")

# feature selection
res.bestglm <- bestglm(Xy = datclean, family = binomial,
            IC = "AIC",
            method = "exhaustive")
#summary(res.bestglm) # printing the summary
res.bestglm$BestModels
#summary(res.bestglm$BestModels)
unloadPkg("bestglm") 
```

The best model based off this feature selection is every feature except video amount. Now we can make that model.

```{r, results='markup'}
typeLogit <- glm(y ~ Age + Gender + MaturitySize + FurLength + Vaccinated + AdoptionSpeed + PhotoAmt, data = datclean, family = "binomial")
xkabledply(typeLogit, title = paste("Logistic Regression : Type ~ age+gender+size+fur+vaccinated+AdoptSpd+photos"))
```

There are a number of significant values for these coefficients. But let's look at other metrics to get a sense of the effectiveness of this model.

```{r, results='markup'}
loadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
typeHoslem = hoslem.test(datclean$y, fitted(typeLogit)) # Hosmer and Lemeshow test, a chi-squared test
typeHoslem
unloadPkg("ResourceSelection") # function hoslem.test( ) for logit model evaluation
```

The GOF value is very low which suggests that the model isn't a great fit. But we can look closer at this with an ROC plot.

```{r, results='markup'}
loadPkg("pROC") # receiver operating characteristic curve, gives the diagnostic ability of a binary classifier system as its discrimination threshold is varied. The curve is on sensitivity/recall/true-positive-rate vs false_alarm/false-positive-rate/fall-out.
prob=predict(typeLogit, type = "response" )
datclean$prob=prob
h <- roc(y~prob, data=datclean)
plot(h)
auc(h)
```

The ROC plot confirms that this is not a great model. The AUC value is `format(auc(h))`, not at our bar of 0.8. Let's check one more measure.

```{r, results = 'markup'}
loadPkg("pscl") # use pR2( ) function to calculate McFadden statistics for model eval
typePR = pR2(typeLogit)
typePR
unloadPkg("pscl") 
```

According to the McFadden score only `r format(round(typePR['McFadden'] * 100), 0)`% of the variance in Type is explained by this model, this is incredibly low. Clearly the logistic regression model is not great at classifying type, or type is not decodeable from the profile.

Lets try a KNN model next. First the variables need to be returned to integers to pass into the model and the numerical features need to be scaled. First we're using all the features and running a search for the optimal K. The search for K went over values 3 - 20.

```{r}
loadPkg("gmodels")

# reloading the original dataframe to cancel all the factor variables
knndat = read.csv('datafile.csv')
knndat = subset(knndat, knndat$Quantity == 1)
knndat = subset(knndat, ! knndat$AdoptionSpeed == 4)

knndat = knndat[c('Type', 'Age', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt','VideoAmt', 'AdoptionSpeed')]

scaledknn = as.data.frame(scale(knndat[c(2, 7, 8)], center = TRUE, scale = TRUE)) # only scaling the numerical values
scaledknn[c('Type', 'Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'AdoptionSpeed')] = knndat[c(1, 3, 4, 5, 6, 9)]

#sampling for train and test
set.seed(1000)
knn_sample <- sample(2, nrow(scaledknn), replace=TRUE, prob=c(0.7, 0.3))

train <- scaledknn[knn_sample==1, 1:3, 5:9]
test <- scaledknn[knn_sample==2, 1:3, 5:9]

typeknn.trainLabel = scaledknn[knn_sample==1, 4]
typeknn.testLabel = scaledknn[knn_sample==2, 4]

ResultDf = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

for (kval in 3:20) {
  tpred <- knn(train = train, test = test, cl=typeknn.trainLabel, k=kval)
  cross <- CrossTable(typeknn.testLabel, tpred, prop.chisq = FALSE)
  cm = confusionMatrix(tpred, reference = as.factor(typeknn.testLabel)) # from caret library
  
  cmaccu = cm$overall['Accuracy']
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  ResultDf = rbind(ResultDf, cmt)
}

ResultDf[which.max(ResultDf$Total.Accuracy),]
```

According to the KNN model with all features included, the best K value is 20 and it produces an accuracy of `r ResultDf[which.max(ResultDf$Total.Accuracy),]$Total.Accuracy`.

Let's try and remove some features. We removed features by logic. For example gender is something that is consistent between types and video amount was removed in the logistic model so we removed it for this test to see if it improved anything. Again we ran through 18 possible K values from 3-20.

```{r}

knndat2 = read.csv('datafile.csv')
knndat2 = subset(knndat2, knndat2$Quantity == 1)
knndat2 = subset(knndat2, ! knndat2$AdoptionSpeed == 4)
knndat2 = knndat2[c('Type', 'Age', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'AdoptionSpeed')]

scaledknn2 = as.data.frame(scale(knndat2[c(2, 6)], center = TRUE, scale = TRUE)) # only scaling the numerical values
scaledknn2[c('Type', 'MaturitySize', 'FurLength', 'Vaccinated', 'AdoptionSpeed')] = knndat2[c(1, 3, 4, 5, 7)]

#sampling for train and test
set.seed(1000)
knn_sample2 <- sample(2, nrow(scaledknn2), replace=TRUE, prob=c(0.7, 0.3))

train2 <- scaledknn2[knn_sample2==1, 1:2, 4:7]
test2 <- scaledknn2[knn_sample2==2, 1:2, 4:7]

typeknn2.trainLabel = scaledknn2[knn_sample2==1, 3]
typeknn2.testLabel = scaledknn2[knn_sample2==2, 3]

ResultDf2 = data.frame( k=numeric(0), Total.Accuracy= numeric(0), row.names = NULL )

for (kval in 3:20) {
  tpred <- knn(train = train2, test = test2, cl=typeknn2.trainLabel, k=kval)
  cross <- CrossTable(typeknn2.testLabel, tpred, prop.chisq = FALSE)
  print( paste("k = ", kval) )
  cm = confusionMatrix(tpred, reference = as.factor(typeknn2.testLabel)) # from caret library
  cmaccu = cm$overall['Accuracy']
  cmt = data.frame(k=kval, Total.Accuracy = cmaccu, row.names = NULL ) # initialize a row of the metrics 
  ResultDf2 = rbind(ResultDf2, cmt)
}

ResultDf2[which.max(ResultDf2$Total.Accuracy),]

```

Not quite as good as the first model so we'll stick with that one, all features and a K of 20.

```{r}

```


# Alexis's section

- SMART Question:  Can puppies/kittens be identified based on their adoption profile?

- Creating section specific variables

```{r}
datage <- dat[ ,c('Type','Gender', 'MaturitySize', 'FurLength', 'Vaccinated', 'PhotoAmt', 'VideoAmt', 'AdoptionSpeed') ]
datage$puppy = NaN
datage$puppy[dat$Age <= 3] = 1
datage$puppy[dat$Age > 3] = 0
datage$puppy = as.factor(datage$puppy)
puppies = subset(datage, datage$puppy == 1)
older = subset(datage, datage$puppy ==0)
puppy_proportion = nrow(puppies) * 100/nrow(dat)
```

- To answer the SMART question, let us consider the general problem. There is a categorical dependent variable, and a mixture of numerical and categorical independent variables. A Logistic Regression makes a great deal of sense in this situation. Potentially KNN or classification-tree model.

As `r puppy_proportion` percent of the pets are puppies, there is a very good balance for our dependent variable. 

## Logistic Model

- Starting off with a full model, excluding Age as it would perfectly predict `r datage$puppy`

```{r}
loadPkg("bestglm")
loadPkg("leaps")
datage$y <- datage$puppy
datage <- na.omit(datage)
logit_pups <- bestglm(Xy = datage, family = binomial, IC = "AIC", method = "exhaustive")
summary(logit_pups)
logit_pups$BestModels
summary(logit_pups$BestModels)
```

## Classification Tree

# Conclusion